[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aaron N’s Blog",
    "section": "",
    "text": "Using Data to Inform Decision Making for Rural Grocery Stores\n\n\n\n\n\n\n\nFinal\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nAaron Null, Alex Cory, Srika Raja and Harun Celik\n\n\n\n\n\n\n  \n\n\n\n\nGrocery Team Weekly Wrap-Up: Week 6\n\n\n\n\n\n\n\nWeek Six\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2023\n\n\nAaron Null, Alex Cory, Srika Raja and Harun Celik\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post - Week 4\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nAaron Null\n\n\n\n\n\n\n  \n\n\n\n\nGrocery Weekly Wrap Up\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nAaron Null, Alex Cory, Srika Raja and Harun Celik\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post - Week 2\n\n\n\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nAaron Null\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post - Week 1\n\n\n\n\n\n\n\nWeek One\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nAaron Null\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Aaron-Null-DSPG-Week-2/Week-Two.html",
    "href": "posts/Aaron-Null-DSPG-Week-2/Week-Two.html",
    "title": "Blog Post - Week Two",
    "section": "",
    "text": "We took some time to explore the TidyCensus package yesterday. Here are some interesting plots:\n\n#install.packages(\"tidycensus\")\n#install.packages(\"tidyverse\")\nlibrary(tidycensus)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(stringr)\nlibrary(patchwork)\n\n\n#Vacancy for county\n\nCounties_Occupancy &lt;- get_decennial(\n  geography = \"county\",\n  year = 2020,\n  state = \"IA\",\n  variables = c(vacant_households = \"H1_003N\",\n                total_households = \"H1_001N\"),\n  output = \"wide\"\n)\n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\n\nNote: 2020 decennial Census data use differential privacy, a technique that\nintroduces errors into data to preserve respondent confidentiality.\nℹ Small counts should be interpreted with caution.\nℹ See https://www.census.gov/library/fact-sheets/2021/protecting-the-confidentiality-of-the-2020-census-redistricting-data.html for additional guidance.\nThis message is displayed once per session.\n\nIowa_Counties &lt;- Counties_Occupancy %&gt;%\n  filter(GEOID %in% c(19019, 19037, 19075, 19169))\n\n\n#Vacancy for cities\n\nCities_Occupancy &lt;- get_decennial(\n  geography = \"place\",\n  year = 2020,\n  state = \"IA\",\n  variables = c(vacant_households = \"H1_003N\",\n                total_households = \"H1_001N\"),\n  output = \"wide\"\n)\n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\nIowa_Cities &lt;- Cities_Occupancy %&gt;%\n  filter(GEOID %in% c(\"1938100\", \"1956100\", \"1933195\", \"1973515\"))\n\n# Combine\n\nIowa_Places &lt;- rbind(Iowa_Cities, Iowa_Counties)\n\n\n# Adding percent columns\n\nIowa_Places &lt;- Iowa_Places %&gt;%\n  mutate(percent = 100 * vacant_households/total_households,\n         occupied_percent = 100 - percent)\n\nIowa_Places &lt;- Iowa_Places %&gt;%\n  mutate(total_percent = 100)\n\n# Plot\n\nIowa_Places %&gt;%\n  ggplot(aes(x = NAME)) +\n  geom_col(aes(y = total_percent, fill = \"Occupied\"), width = 0.8) + \n  geom_col(aes(y = percent, fill = \"Vacant\"), width = 0.8) +\n  coord_flip()\n\n\n\n\nThis is a graph representing the percentage of vacant housing units per location for both the counties and their respective cities. Slater City has the lowest percentage of vacancies while New Hampton City has the highest.\nI then looked at median earnings within the last 12 months:\n\n#API Call\n\nmedian_earnings &lt;- get_acs(geography = \"county\",\n                       variables = \"B08521_001\",\n                       state = \"IA\",\n                       county = c(\"Buchanan County\", \"Chickasaw County\", \"Grundy County\", \"Story County\"),\n                       year = 2021,\n                       survey = \"acs5\")\n\nGetting data from the 2017-2021 5-year ACS\n\nmedian_earnings &lt;- get_acs(geography = \"county\",\n                       variables = \"B08521_001\",\n                       state = \"IA\",\n                       county = c(\"Buchanan County\", \"Chickasaw County\", \"Grundy County\", \"Story County\"),\n                       year = 2021,\n                       survey = \"acs5\")\n\nGetting data from the 2017-2021 5-year ACS\n\nmedian_earnings\n\n# A tibble: 4 × 5\n  GEOID NAME                   variable   estimate   moe\n  &lt;chr&gt; &lt;chr&gt;                  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1 19019 Buchanan County, Iowa  B08521_001    35810  1743\n2 19037 Chickasaw County, Iowa B08521_001    38946  3114\n3 19075 Grundy County, Iowa    B08521_001    43307  1966\n4 19169 Story County, Iowa     B08521_001    38214  1577\n\n\n\nmedian_earnings %&gt;%\n  ggplot(aes(x = NAME, y = estimate)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = estimate - moe, ymax = estimate + moe), width = 0.2, position =    position_dodge(0.9)) +\n  ylab(\"Median Earnings\") + \n  xlab(\"County\")\n\n\n\n\nThis graph represents the median of individuals’ earnings within the last 12 months by county. Grundy County reports the highest while Buchanan County reports the lowest.\nThen I looked at the different languages spoken in each county:\n\n# List of language variables (provided by Chris Seeger)\n\nlangList = c(\"Speak only English\" = \"C16001_002\",   \"Spanish\" = \"C16001_003\",   \"French, Haitian, or Cajun\" = \"C16001_006\",   \"German or other West Germanic languages\" = \"C16001_009\",   \"Russian, Polish, or other Slavic languages\" = \"C16001_012\",   \"Other Indo-European languages\" = \"C16001_015\",   \"Korean\" = \"C16001_018\",   \"Chinese (incl. Mandarin, Cantonese)\" = \"C16001_021\",   \"Vietnamese\" = \"C16001_024\",   \"Tagalog (incl. Filipino)\" = \"C16001_027\",   \"Other Asian and Pacific Island languages\" = \"C16001_030\",   \"Arabic\" = \"C16001_033\",   \"Other and unspecified languages\" = \"C16001_036\")\n\n#API Call\n\nlang &lt;- get_acs(geography = \"county\",\n                       variables = langList,\n                       state = \"IA\",\n                       year = 2021,\n                       survey = \"acs5\")\n\nGetting data from the 2017-2021 5-year ACS\n\n# Subsetting data with string detection\n\nlang_counties &lt;- lang %&gt;%\n  filter(str_detect(NAME, \"Buchanan|Chickasaw|Grundy|Story\"))\n\n\n# Plots\nplot_1 &lt;- lang_counties %&gt;%\n  ggplot(aes(x = NAME, y = estimate, fill = variable)) + \n  geom_bar(stat = \"identity\", position = \"fill\") + \n  xlab(\"County\") +\n  ylab(\"Languages\") +\n  coord_flip()\n\nplot_2 &lt;- lang_counties %&gt;%\n  filter(variable != \"Speak only English\") %&gt;%\n  ggplot(aes(x = NAME, y = estimate, fill = variable)) + \n  geom_bar(stat = \"identity\", position = \"fill\") + \n  xlab(\"County\") +\n  ylab(\"Language (Other Than English)\") +\n  coord_flip()\n\nplot_1 \n\n\n\n\nThis plot shows the distribution of languages spoken in the four counties with English included.\n\nplot_2\n\n\n\n\nThis plot shows the language distribution excluding English. There is a great degree of variation from county to county in regards to the distribution of spoken languages other than English (which comprises a vast majority in all of the counties).\nSimilar plots were made for race/ethnicity.\n\n# Created variable list\n\nraceList = c(\"White\" = \"P1_003N\", \"Black\" = \"P1_004N\", \"American Indian/Alaskan Native\" = \"P1_005N\", \"Asian\" = \"P1_006N\", \"Native Hawaiian/Pacific Islander\" = \"P1_007N\", \"Other\" = \"P1_008N\")\n\nrace_counties &lt;- get_decennial(geography = \"county\",\n                       variables = raceList,\n                       state = \"IA\",\n                       county = c(\"Buchanan County\", \"Chickasaw County\", \"Grundy County\", \"Story County\"),\n                       year = 2020)\n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\n\n\nplot_1 &lt;- race_counties %&gt;%\n  ggplot(aes(x = NAME, y = value, fill = variable)) + \n  geom_bar(stat = \"identity\", position = \"fill\") + \n  xlab(\"County\") +\n  ylab(\"Race/Ethnicity\") +\n  coord_flip()\n\nplot_2 &lt;- race_counties %&gt;%\n  filter(variable != \"White\") %&gt;%\n  ggplot(aes(x = NAME, y = value, fill = variable)) + \n  geom_bar(stat = \"identity\", position = \"fill\") + \n  xlab(\"County\") +\n  ylab(\"Race/Ethnicity (Other than White)\") +\n  coord_flip()\n\nplot_1\n\n\n\n\nThis is a plot of the distribution of race/ethnicity for each of the 4 counties.\n\nplot_2\n\n\n\n\nThis is a plot of of the distribution of racial minorities in each county.\nIt was also important to find the Hispanic/Latino population figures for each county, as it wasn’t clear how that group was being incorporated into the data used in the plots above.\n\n# API Call\nhispanic_counties_wide &lt;- get_decennial(geography = \"county\",\n                       variables = c(\"Hispanic/Latino\" = \"P2_002N\", \"Not Hispanic/Latino\" = \"P2_003N\"),\n                       state = \"IA\",\n                       county = c(\"Buchanan County\", \"Chickasaw County\", \"Grundy County\", \"Story County\"),\n                       year = 2020,\n                       output = \"wide\")\n\nGetting data from the 2020 decennial Census\n\n\nUsing the PL 94-171 Redistricting Data summary file\n\n# Table\nhispanic_counties_wide &lt;- hispanic_counties_wide %&gt;%\n  mutate(percentage = 100 * `Hispanic/Latino`/`Not Hispanic/Latino`)\n\nhispanic_counties_wide\n\n# A tibble: 4 × 5\n  GEOID NAME                  `Hispanic/Latino` `Not Hispanic/Latino` percentage\n  &lt;chr&gt; &lt;chr&gt;                             &lt;dbl&gt;                 &lt;dbl&gt;      &lt;dbl&gt;\n1 19019 Buchanan County, Iowa               338                 20227       1.67\n2 19037 Chickasaw County, Io…               481                 11531       4.17\n3 19075 Grundy County, Iowa                 145                 12184       1.19\n4 19169 Story County, Iowa                 5032                 93505       5.38\n\n\nThis table shows the reported Hispanic/Latino population for 2020 for each of the 4 counties, along with percentages.\nI decided to turn my attention to Iowa at large and look at poverty rates.\n\n# Plot\nggplot(poverty) +\n  geom_sf(aes(fill = estimate)) + # Fill the counties by population density\n  scale_fill_distiller(palette = \"YlOrBr\") + # Use a color-blind friendly palette\n  labs(title = \"Individuals Below the Poverty Line in Iowa\",\n       subtitle = \"Source: American Community Survey 2021\",\n       fill = \"People per Block Group\") + # Add labels\n  theme_minimal()\n\n\n\n\nThis is a plot of the number of individuals per block group who earned low enough wages over 12 months to place them below the poverty line in 2021. The rate differs more or less uniformly around the state save for a few key regions.\nFinally, I looked at the reported mode of transport to work for each of the four counties. A similar stacked proportional bar graph was made to the ones for language and race above.\n\n# Compiled variable list for mode of transport\ntransportList = c(\"Automobile\" = \"B08006_002\",\"Bus\" = \"B08006_009\", \"Subway/Elevated Rail\" = \"B08006_010\", \"Train\" = \"B08006_011\",\"Light Rail\" = \"B08006_012\",\"Ferryboat\" = \"B08006_013\", \"Bicycle\" = \"B08006_014\",\"Walked\" = \"B08006_015\", \"Taxi/Motorcycle/Other\" = \"B08006_016\")\n\n# API Call\n\ntransport_counties &lt;- get_acs(geography = \"tract\",\n                       variables = transportList,\n                       state = \"IA\",\n                       year = 2021,\n                       survey = \"acs5\")\n\nGetting data from the 2017-2021 5-year ACS\n\n# Subset for relevant counties using str_detect()\ntransport_counties2 &lt;- transport_counties %&gt;%\n  filter(str_detect(NAME, \"Buchanan|Chickasaw|Grundy|Story\"))\n\n# Mutating 'county' column and labeling for each tract observation\ntransport_small &lt;- transport_counties2 %&gt;%\n  mutate(county = ifelse(str_detect(NAME, \"Buchanan\"), \"Buchanan County\",\n                    ifelse(str_detect(NAME, \"Chickasaw\"), \"Chickasaw County\",\n                      ifelse(str_detect(NAME, \"Grundy\"), \"Grundy County\", \"Story County\"))))\n\n\nplot_1 &lt;- transport_small %&gt;%\n  ggplot(aes(fill = variable, x = county, y = estimate)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  coord_flip() + \n  ggtitle(\"Mode of Transport to Work\")\n\nplot_2 &lt;- transport_small %&gt;%\n  filter(variable != \"Automobile\") %&gt;%\n  ggplot(aes(fill = variable, x = county, y = estimate)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  coord_flip() +\n  ggtitle(\"Mode of Transport to Work (Other than Automobile)\")\n\nplot_1\n\n\n\n\nThis is a plot of the reported mode of transport to work for each of the 4 counties. Commute by automobile overwhelmingly predominates.\n\nplot_2\n\n\n\n\nThis is a plot of the reported mode of transport for each county with automobile excluded. In two of the counties, “walking” is the majority response.\n\n\nI learned a lot by exploring this data, both about the census data itself and about different means of data wrangling in R. One of the biggest takeaways for me was being introduced to the “stringr” package and learning how to detect key words in strings. Along with that, I feel I have a clearer understanding of the “ifelse()” function and how it works. I also remembered how to add error bars to plots and how to make maps given the presence of the appropriate data. I’m looking forward to exploring these data sets even more and hope they will prove to be useful for our project."
  },
  {
    "objectID": "posts/Aaron-Null-DSPG-Week-2/Week-Two.html#conclusion",
    "href": "posts/Aaron-Null-DSPG-Week-2/Week-Two.html#conclusion",
    "title": "Blog Post - Week Two",
    "section": "",
    "text": "I learned a lot by exploring this data, both about the census data itself and about different means of data wrangling in R. One of the biggest takeaways for me was being introduced to the “stringr” package and learning how to detect key words in strings. Along with that, I feel I have a clearer understanding of the “ifelse()” function and how it works. I also remembered how to add error bars to plots and how to make maps given the presence of the appropriate data. I’m looking forward to exploring these data sets even more and hope they will prove to be useful for our project."
  },
  {
    "objectID": "posts/Aaron-Null-DSPG-Week-One/Week-One.html",
    "href": "posts/Aaron-Null-DSPG-Week-One/Week-One.html",
    "title": "Blog Post - Week 1",
    "section": "",
    "text": "This week was all about becoming more familiar with the data science workflow and understanding our projects. We also became acquainted with the history of DSPG and some of its past projects.\nWe started the first day doing an exercise where we had 90 minutes to analyze flight data from the Des Moines airport.\n\n\n\nTotal Number of Flight by Airline (Following Thanksgiving 2021)\n\n\n\n\n\nNumber of Des Moines Flights by Airline Across Time (Following Thanksgiving 2021)\n\n\nThis exercise was a good warm-up into using ggplot2 again and remembering which figures and summary statistics to prioritize reporting.\nWe also listened to many interesting speakers, including Chris, Sadat and Cass Dorius at the research park.\n\n\n\nGithub Concepts\nUnderstanding Data Visualization"
  },
  {
    "objectID": "posts/Aaron-Null-DSPG-Week-One/Week-One.html#datacamp",
    "href": "posts/Aaron-Null-DSPG-Week-One/Week-One.html#datacamp",
    "title": "Blog Post - Week 1",
    "section": "",
    "text": "Github Concepts\nUnderstanding Data Visualization"
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#our-tools",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#our-tools",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Our Tools",
    "text": "Our Tools\n\nR\nOne of the first decisions made was our choice of programming language. We decided to choose to use the R programming language for this project. Created by statisticians in 1993, R is a popular and effective language for data science and statistics. In addition, R has many great libraries for data visualization, such as ggplot2, that can help us effectively show relevant demographic data related to potential markets.\n\n\nR Shiny\nOne of the advantages of R is that it grants us access to R Shiny, a package that allows for the creation of dynamic, easy-to-use data-driven web applications. Shiny apps can be easily incorporated into web pages and can take a variety of different types of user input. In addition, Shiny is highly compatible with packages such as leaflet for map displays and spatial analysis.\n\n\n\nCensus Data\nIn addition, R gives us access to the TidyCensus, a package that makes it very easy to retrieve Census data for any location in the United States. TidyCensus acts as a channel for extracting specific data from the US Census Bureau database in a tidy format. This Census data is important for both our calculations and our data visualizations.\n\n\nTigris\nTigris is a package designed to help developers use Census Bureau TIGER/Line files. This is being used to find county and place info, and is being used to determine which cities are in the market area.\n\n\nGoogle Places API\nThe Google Places API is being used to find nearby stores, and to convert addresses to GPS coordinates. These stores help to determine our market size (“buffer zone”) and factor in the effect on competition on store profitability."
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#population-functions",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#population-functions",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Population Functions",
    "text": "Population Functions\n\nMetro Population\nMetro Population is a function that determines the population of the city that the potential store would be in. This function uses the parsed address to select just the city the store is out of a data frame containing the population of all the towns in the county that the store is in.\n\n\nCities Population\nCities Population is a function that finds all the nearby cities and finds the total of their populations. This function uses a TidyCensus call to find the population of all the cities in the state, and then filters it down to be just the cities in our area. It then takes the sum of all the city populations.\n\n\nRural Population\nRural_Pop is a function that finds the population of all people who live in the market area but do not live in a town. This is done by finding the population of the whole county, and subtracting the people who live in all the towns in that area, then multiplying by the percentage of the county the area covers.\nThe area calculations used in our functions are derived by finding the distance to the nearest store in each quadrant (NE, NW, SE, SW). We use that distance as the radius of a quarter circle to figure out how much reach the store has. This is a rudimentary way of determining the market region as the only thing it takes into account is distance to the closest store. This method ignores the radius that the other store would have. A more elegant solution to this would be Voronoi polygons, Reilly’s law of Retail Gravitation, or Huff’s Model."
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#future-potential-steps",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#future-potential-steps",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Future potential steps",
    "text": "Future potential steps\n\nVoronoi\nA Voronoi diagram is a way of partitioning a plane into cells where all points within the cell are closer to a given seed, or in our case a store, than any other seed. Implementing these would be beneficial in order to get a more accurate model that would account for the other stores market area.\n\n\n\nReilly’s\nReilly’s law is an economic principle that states that people are more drawn to areas with a higher population than those with smaller populations. This is an excellent way to estimate market area, but is somewhat complicated. Maps are usually edited by hand to account for geographic barriers such as rivers, and there are various limitations such as the populations of the two cities having to be relatively similar.\n\n\nHuff’s Model\nHuff’s law is a probabilistic model for estimating consumer attraction. It states that the attractiveness of a store and the something called distance decay determine the likelihood of someone visiting the store. Distance decay is the idea that as people move farther and farther away, the likelihood exponentially decreases that they would visit the store. In the real world this would mean that even though a store might be 10 times more attractive than another, if it is 10 times farther away, they most likely will not travel to it."
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#bizminervertical-iq-percentages",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#bizminervertical-iq-percentages",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Bizminer/Vertical IQ Percentages",
    "text": "Bizminer/Vertical IQ Percentages\nMany of the functions used in this package incorporate percentages taken from both Bizminer and Vertical IQ, two financial analysis and market research firms. The value of these percentages determine how much of the total estimated revenue will be spent on a given category of expense. These percentages are based off of industry 3-year averages for grocery stores as of 2022. In this tool, these default percentages are the averages taken from Bizminer and Vertical IQ’s individual 3-year averages. Although the user has the freedom to choose their own percentage via sliders in the side panel, these Bizminer/Vertical IQ percentages are set as defaults."
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#categories-of-expenses",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#categories-of-expenses",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Categories of Expenses",
    "text": "Categories of Expenses\nFive different categories of expenses are accounted for in this tool:\n\nCost of Goods Sold\nThis represents how much of the total estimated revenue is spent on the cost of goods (inventory). The calculation for this item takes the total estimated revenue and the selected gross margin percentage into account.\nAssociated Functions:\n\nGross_Margin()\nCost_of_Goods_Sold()\n\n\n\nOperating Expenses\nComprises expenses such as compensation for company officers, employee wages and other common operating expenses of grocery stores. Users can select percentages for each of these expenses or they can rely on the default value provided by Bizminer.\nAssociated Functions:\n\nEmployee_Wages()\nOfficer_Compensation()\nOther_Operating_Expense()\n\n\n\nAsset Depreciation\nCovers the annual loss from the depreciation of various assets required to own and operate a grocery store. The value of a list of assets are divided by their associated use life to determine the annual lost in value from asset depreciation. The tool gives the user the option to fill out a form for one of two scenarios: one if the user plans to own a building and the other if the user plans to rent.\nAssociated Functions:\n\nDepreciation_1()\nDepreciation_2()\n\n\n\nLoan Interest\nCovers the annual interest on a user-specified loan (if a loan was taken out). The annual cost of interest on a loan is calculated from the loan amount and its annual interest rate, entered by the user.\nAssociated Function:\n\nInterest_Expense()\n\n\n\nRent (if leasing building)\nThis covers the annual cost of rent for a leased building. Users can enter their monthly rate of rent to determine the annual cost of rent for their building.\nAssociated Function:\n\nAnnual_Rent()"
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#secondary-sources-of-income",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#secondary-sources-of-income",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Secondary Sources of Income",
    "text": "Secondary Sources of Income\nAnother important factor to consider is secondary sources of income. These are broken up into two main categories:\n\nIncome from Interest\nIncome collected from interest-bearing assets.\nAssociated Function:\n\nInterest_Income()\n\n\n\nOther Income\nThis could include special services outside the revenue from sold goods alone, such as special grocery delivery services or membership fees for discount clubs, among other things.\nAssociated Function:\n\nOther_Income()"
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#pre-tax-profit",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#pre-tax-profit",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Pre-Tax Profit",
    "text": "Pre-Tax Profit\nIn the tool, the estimated expenses will be subtracted from the total estimated revenue and the estimated secondary income will then be added in the calculation for the estimated pre-tax profit . Users will be able to adjust the percentages to suite their individual needs and determine profitability taking both expenses and market size into account."
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#demographic-information",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#demographic-information",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Demographic Information",
    "text": "Demographic Information\nData from the American Community Survey (2021) and the Decennial Census (2020) were pulled in order to display data visualizations relevant to food desert status and the understanding the market of a given location. This data is displayed in three bar plots and one data table inside a dashboard in the tool. Based on the market size around a user-specified address, the data is retrieved for the all of the counties that hold a city that resides within a circular buffer zone around that location.\nKey variables shown for each county in the dashboard:\n\nMedian Household Income: correlates with access to food, provides important information about the store’s potential customer base\nEmployment Status: Indicator of poverty, highlights which areas may benefit most from the additional jobs provided by a potential store\nLanguages Spoken (other than English): Helps highlight the cultural makeup of a market area\nRace/Hispanic Origin: Areas with higher proportions of minority groups are most likely to be living in food deserts\nTotal Population: Foundational for understanding market size (included alongside Race/Hispanic Origin table)\n\nIn addition to purely economic metrics, its important to highlight the relationship between the proportion of racial and ethnic minorities and food desert status of communities. Moreover, having insight into the racial and cultural dynamics of a community allows potential owners to make more informed decisions about where their individual plans for their stores are most likely to succeed (for example, a Mexican immigrant entrepreneur looking to open a small market that caters to Hispanics).\n\n\n\nPlot showing the three most commonly spoken languages of each county other than English (2021)\n\n\nSince the data taken from the American Community Survey is based off of representative sampling, each of the bar plots display corresponding margin of error bars for each county’s estimate. The estimate and the margin of error can also be found by hovering the cursor over each bar. The Race/Hispanic Origin Table, on the other hand, is taken from the Decennial Census and contains counts of the total population, thus lacking these error bars."
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#results",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#results",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#conclusion",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#conclusion",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Conclusion",
    "text": "Conclusion\nThrough continuous improvement, our tool presents a significant opportunity to combat food deserts, giving small rural business owners the means to create sustainable grocery stores and uplift their communities. We can implement better ways to calculate the market size (like Voronoi and Reillys), include start-up costs while estimating the expense, and give users more information about other credible resources that can help them."
  },
  {
    "objectID": "posts/Weekly_Wrap_Up_Week_3/Weekly_Wrap_Up_Week-3.html",
    "href": "posts/Weekly_Wrap_Up_Week_3/Weekly_Wrap_Up_Week-3.html",
    "title": "Grocery Weekly Wrap Up",
    "section": "",
    "text": "This week was primarily centered on clarifying the focus of our project, as well as on the retrieval of relevant sources of data. During the first part of the week, our group searched TidyCensus, the USDA and various other internet sources for data that could potentially be useful for whichever type of tool that we and our client decide on.\nIt could be said that we have started moving into the second phase of our project this week: data collection.\n\nOn Thursday, we met with our client and some of his associates at the the research park to discuss the direction of the project. We had them answer various questions on Mentimeter and showed them the list of datasets under consideration. They also gave us numerous suggestions about sources they were familiar with and told us that they would help grant us access to those of which were not immediately available to us.\n\n\nHere are some of the Mentimeter results from our meeting:\n\n\n\nAs we are wrapping up this week, we have now shifted our focus to compiling our selected data to our repository and filtering through sources that may not be as useful as others. We are continuing to think of ways to optimize this process, as well as remaining open to other sources of data or information that we have yet to discover. Here are some of the resources under consideration:"
  },
  {
    "objectID": "posts/Weekly_Wrap_Up_Week_3/Weekly_Wrap_Up_Week-3.html#aaron",
    "href": "posts/Weekly_Wrap_Up_Week_3/Weekly_Wrap_Up_Week-3.html#aaron",
    "title": "Grocery Weekly Wrap Up",
    "section": "Aaron",
    "text": "Aaron\nThis week has been less coding-centric than the previous week, as many of our efforts have been centered on uncovering useful sources of data. However, I have tried to work through ways to accelerate the time-consuming process of labeling and pushing CSVs of tables of ACS data.\nHere is an example of such a table: “Mode of Transport to Work by County 2021”\n\nI have also written a function that I’ve been using to speed the process up. It’s a data labeler function that takes “table”, “year” and “geography as arguments and outputs a labelled ACS5 data table by joining labels from the”load_variables()” function in TidyCensus by the common variable code on both dataframes.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidycensus)\nlibrary(stringr)\nlibrary(readr)\n\n\n\ndata_labeler_Iowa &lt;- function(table, year, geography){\n  df &lt;- tidycensus::get_acs(geography = geography,\n                table = table,\n                state = \"IA\",\n                year = year,\n                survey = \"acs5\")\n  \n  \n  vars &lt;- tidycensus::load_variables(year, \"acs5\")\n  \n  vars &lt;- vars %&gt;%\n    dplyr::filter(stringr::str_detect(name, table))\n  \n  vars &lt;- vars %&gt;% \n    dplyr::rename(variable = name)\n  \n  vars &lt;- vars %&gt;%\n    dplyr::select(variable, label)\n  \n  df2 &lt;- merge(df,vars)\n  \n  df2 %&gt;%\n    dplyr::arrange(GEOID)\n}\n\nMedian_Household_Income_County_2021 &lt;- data_labeler_Iowa(\"B19013\", 2021, \"county\")\n\nGetting data from the 2017-2021 5-year ACS\n\n\n\nhead(Median_Household_Income_County_2021)\n\n    variable GEOID                   NAME estimate  moe\n1 B19013_001 19001     Adair County, Iowa    57944 4047\n2 B19013_001 19003     Adams County, Iowa    57981 7853\n3 B19013_001 19005 Allamakee County, Iowa    59461 2644\n4 B19013_001 19007 Appanoose County, Iowa    46900 5645\n5 B19013_001 19009   Audubon County, Iowa    54643 5861\n6 B19013_001 19011    Benton County, Iowa    72334 3626\n                                                                                         label\n1 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n2 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n3 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n4 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n5 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n6 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n\n\nBuilding on this, I tried to iterate through all of these arguments by using nested for loops and writing csvs automatically. However, for a reason I don’t entirely understand, I haven’t gotten it to work the way I should. It’s possible that this has to do with the limitations of the API call and its speed within the loop.\n\nyearlist &lt;- c(2009, 2012, 2016, 2021)\ngeo_list &lt;- c(\"county\", \"tract\")\nIncome_table_list &lt;- c(\"B19013\", \"B19113\", \"B19202\", \"B19051\", \"B19055\")\nacs_table &lt;- NULL\n\n\nfor(table in Income_table_list)\n\n  for(year in yearlist)\n  \n    for(geography in geo_list)\n    \n      acs_table &lt;- tidycensus::get_acs(geography = geography,\n                                       table = table,\n                                       state = \"IA\",\n                                       year = year,\n                                       survey = \"acs5\")\n      \n      write_csv(acs_table, file = sprintf(\"%s_%s_%s\", table, year, geography))\n\nHowever, even if this didn’t completely work, I learned a lot of useful information, such as the “sprintf” function that provides placeholders for strings that are named from iterating variables in loop, as well as how to call only necessary elements of a library into a function to prevent using too much memory/slowing run time."
  },
  {
    "objectID": "posts/Weekly_Wrap_Up_Week_3/Weekly_Wrap_Up_Week-3.html#alex",
    "href": "posts/Weekly_Wrap_Up_Week_3/Weekly_Wrap_Up_Week-3.html#alex",
    "title": "Grocery Weekly Wrap Up",
    "section": "Alex",
    "text": "Alex\n\nData Exploration\nI spent a lot of time this week doing data exploration. I found the USDA food atlas data set, as well as the Economic Census data. I also explored the TinyUSDA package.\n\n\nClient Meeting\nThis week I met with our clients, where we clarified the scope of the project, discussed the goals moving forwards, and shared our progress. I shared about the USDA food atlas and Economic Census data sets\n\n\nSQL learning\nThis week I spent time learning PostgreSQL on DataCamp. I also did several other courses related to data visualizations and fundamental statistical skills."
  },
  {
    "objectID": "posts/Weekly_Wrap_Up_Week_3/Weekly_Wrap_Up_Week-3.html#srika",
    "href": "posts/Weekly_Wrap_Up_Week_3/Weekly_Wrap_Up_Week-3.html#srika",
    "title": "Grocery Weekly Wrap Up",
    "section": "Srika",
    "text": "Srika\n\nTraining:\n\nData Camp courses completed:\nIntroduction to statistics in R\nData visualization with R (in progress)\n\n\n\nCreated summaries of the client reports:\nUnderstanding the market trends and margins for different departments in a rural grocery store.\n\n\nCollected some ACS Data\nBrowsed for other possible useful sources to consider"
  },
  {
    "objectID": "posts/Week_4/Week-4.html",
    "href": "posts/Week_4/Week-4.html",
    "title": "Blog Post - Week 4",
    "section": "",
    "text": "Grocery Project:\nThis week was centered on converting the sheets that our client gave us to useful functions for our project package. My functions were R translations of our clients sheet for estimating expenses of potential rural grocery stores.\nSo far, there are 8 functions that are derived from the expense estimation sheet, yet it is still a work in progress. Here are the ones we have so far:\nCost_of_Goods_Sold()\nGross_Margin()\nOfficer_Compensation()\nEmployee_Wages()\nOther_Operating_Expense()\nOperating_Income_Loss()\nAnnual_Rent()\nInterest_Expense()\nMany of these functions give different outputs based on the range that the input for total estimated revenue falls into. Here, the revenue ranges are listed in a table with the corresponding percentages for each variable (in decimal form):\n\nTherefore, some of our functions required a way to account for these differences. As it stands, this is dealt with through nested ifelse statements.\n\nEmployee_Wages &lt;- function(Total_Estimated_Revenue) {\n\n  ifelse(Total_Estimated_Revenue &lt; 500000, stop(\"error: no data for this revenue range\"),\n         percentage &lt;- ifelse(Total_Estimated_Revenue &lt; 999999.99, .0789,\n                              ifelse(Total_Estimated_Revenue &lt; 2499999.99, .0934,\n                                     ifelse(Total_Estimated_Revenue &lt; 4999999.99, .0751,\n                                            ifelse(Total_Estimated_Revenue &lt; 24999999.99, .0975, .1083)))))\n\n\n\n  Total_Estimated_Revenue * percentage\n}\n\nIt’s likely that more work needs to be done for error handling in functions like these.\nHere’s a function that depends on some other functions in the package:\n\nOperating_Income_Loss &lt;- function(Total_Estimated_Revenue) {\n  Goods_cost &lt;- Cost_of_Goods_Sold(Total_Estimated_Revenue)\n  Officer_comp &lt;- Officer_Compensation(Total_Estimated_Revenue)\n  Employee_Wages &lt;- Employee_Wages(Total_Estimated_Revenue)\n  Other_Expense &lt;- Other_Operating_Expense(Total_Estimated_Revenue)\n\n  Total_Estimated_Revenue - sum(Goods_cost, Officer_comp, Employee_Wages,\n                                Other_Expense)\n}\n\nMost of the functions only take one argument: Total Estimated Revenue. This will be calculated from the functions made from our revenue estimation sheet (Srika). Other functions, such as Annual_Rent() or Interest_Expense(), have inputs that are dependent upon a user’s individual scenario, such as the size of a loan or the rate of monthly rent for the building.\nThere are still questions left to be answered, such as how to precisely estimate the cost of depreciation and how to determine the correct use life of certain assets. However, our team all made significant progress this week.\n\n\nCoffee Talk\nI also learned a lot about text mining in R this week. I decided to compare the reviews from two gaming journalistic outlets for the past 5 Legend of Zelda games using certain text mining techniques. Some of the interesting libraries I learned about include:\n\ntm\ntidytext\nRWeka\nqdap\nplotrix\nrvest\nwordcloud\nsyuzhet\n\nHere are some plots from the project:\n\n\n\nWordcloud of the whole Zelda series\n\n\n\n\n\nPyramid Plot of IGN vs. Gamespot\n\n\n\n\n\nSentiments for Tears of the Kingdom\n\n\nMany of the techniques used for the project was from the Datacamp course “Text Mining with Bag of Words in R” and “Sentiment Analysis in R”.\nOverall, today has been a pretty good week."
  },
  {
    "objectID": "posts/Week_6/Week-6.html",
    "href": "posts/Week_6/Week-6.html",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "",
    "text": "Lunchtime at Grundy Center."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#grocery-survey-first-impressions",
    "href": "posts/Week_6/Week-6.html#grocery-survey-first-impressions",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Grocery Survey: First Impressions",
    "text": "Grocery Survey: First Impressions\nWe have also been looking over the results of a survey conducted by Iowa State Extension and Outreach (sponsored by AgMRC) that asked owners and senior managers of independently-owned rural grocery stores throughout Iowa a series of questions about their store’s performance, selection and design, as well as the top challenges they face in keeping their store profitable. The survey divided Iowa into 3 regions and examined the survey results accordingly:\n\n\n\nThree regions analyzed by the survey.\n\n\nOne issue with the survey was the low response rate: only 95 out of 671 independent grocers responded (14.2%). However, the findings might still be useful to look at for our purposes.\nHere are some charts I found interesting:\n\n\n\nTop challenges faced by small grocers in Iowa\n\n\n\n\n\nDifficulties in purchasing and selling local products\n\n\nThere is much more potentially useful information, such as information about the most frequently found unique assets of stores and the specific types of local products most frequently bought according to region, shown here:\n\n\n\nType of local foods purchased by region\n\n\nOverall, this survey may help us provide useful information to the users of our tool and can help grant us perspective on how well the functionality of our tools maps onto the ground truth of what is really going on with rural grocery stores in Iowa. I found it to be a very interesting and useful resource.\n\nBizminer Percentages\nOne important concern for the expenses component of the project is the question of modularity, or the freedom given to the user to affect the calculations performed in the server. The functions used in this project to calculate expenses for potential grocery stores do so by providing different percentages of revenue spent on expenses based off of how high the total estimated revenue input is at the beginning. The original excel sheet establishes 5 distinct ranges that each have different percentages of the revenue across the different line items of the sheet.\n\n\n\nPercentages for categories of expenses/income across revenue ranges/store sizes (Bizminer)\n\n\nThese ranges can be thought of as stand-ins for the different sizes of grocery stores. When a store takes in a certain amount of revenue, the function assigns a budget percentage corresponding to the assumed size of the store.\nAs it stands now, many of the functions make use of nested “ifelse” statements in order to find the budget percentage of an expense or a source of income and multiply it by the total estimated revenue. For example:\n\nEmployee_Wages &lt;- function(Total_Estimated_Revenue) {\n\n  ifelse(Total_Estimated_Revenue &lt; 500000, stop(\"error: no data for this revenue range\"),\n         percentage &lt;- ifelse(Total_Estimated_Revenue &lt; 999999.99, .0789,\n                              ifelse(Total_Estimated_Revenue &lt; 2499999.99, .0934,\n                                     ifelse(Total_Estimated_Revenue &lt; 4999999.99, .0751,\n                                            ifelse(Total_Estimated_Revenue &lt; 24999999.99, .0975, .1083)))))\n\n  Total_Estimated_Revenue * percentage\n}\n\nThe function gives the user an error message if they enter a revenue amount under $500,000 dollars due to there being no data at that range.\n\n\nThe Linearity Finding\nMany of the functions derived from the excel sheet “Estimating Expenses” have been written to take the total estimated revenue as an input and return the dollar amount of a certain cost in accordance to the average percentage of revenue spent on that cost for stores of a similar size. For example, a store that takes in $1,000,000 and a store that takes in $2,000,000 share common percentages for costs such as employee wages and officer compensation. This broad assignment of one percentage based on a large revenue window also applies to the function for calculating gross margin, or the amount of funds left over after accounting for the cost of goods for the store.\nHowever, since most of these functions receive the same input of total estimated revenue and all of the percentages for the expenses are bundled together for the same window of revenue, this creates a problem. Suppose we want to compare the profitability of a store that takes in $1,000,000 of revenue vs. $2,000,000. According to the way our calculations work now, both of these stores will have the same percentage of their revenue deducted as expenses. So if 95% of our revenue goes towards covering expenses for every value that falls within the window of $1,000,000 to $2,500,000, there is a perfect linear relationship between the revenue and the profit of the store; the $1,000,000 store collects $50,000 in profit while the $2,000,000 store collects $100,000. If we were to create a numeric regular sequence of revenue inputs from $1,000,000 to $2,500,000 and plot it against one of our measures of profit (before depreciation etc.), we would get:\n\n\n\nRevenue vs. Expense\n\n\nThis is a problem because the ultimate goal of our tool is to help potential grocery store owners decide which area could be more profitable for the store that they want to open than the next area. In other words, the model doesn’t afford any insight into the relative advantage of setting up a store in a given a area versus another, because any increased revenue from a chosen area is immediately offset by a corresponding increase in expenses that affects every store within a revenue range uniformly.\n\n\nA Potential Solution\nOne way to ameliorate this problem is to simply give more control to the user of the application in choosing their own expense percentages. That way, the user can tinker with the tool in order to determine (in a broad way) how much resources they must have and how they must allocate those resources in order to be profitable in a certain location. We propose that this can be done through R Shiny. R Shiny allows for numeric inputs into functions by means of a user-controlled slider. Through this method, there is no need to discard the default percentages that we were previously working with; rather, instead of being hard-coded into the function, they can be placed as labels along the slider or set as default values that the user can adjust manually if they so choose.\n\n\n\nSlider input example (Shiny)\n\n\nThis way, the user can know the industry averages for different variables while also comparing their own plans with those averages.\n\n\nThe Rounding Problem\nAnother issue I faced was one involving the rounding of percentages of my functions. I had rounded the original percentages up or down depending on whether they were considered a cost or an expense in the balance sheet, and this affected my output significantly when I went to compare my R output to the example provided by Duane (our client) in his excel sheet. Thankfully, when I reverted the percentages to their original state (4 decimal places), my outputs then matched the one’s displayed in the excel sheet. The big takeaway from this is that most of the time, the final output is what should be rounded in a calculation and not the intermediaries. I mistakenly thought that rounding the percentages used in the calculation in certain directions could provide a more “conservative” or “risky” weight to the final prediction. However, since we were dealing with small percentages of very large numbers, the degree of rounding that was there greatly impacted overall accuracy and rendered the predictions illogical. Thankfully, the functions work as they are supposed to now (even thought they may undergo a significant rewrite later)."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#geocodingreverse-geocoding",
    "href": "posts/Week_6/Week-6.html#geocodingreverse-geocoding",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Geocoding/Reverse Geocoding",
    "text": "Geocoding/Reverse Geocoding\nAlongside working on the expenses functions, I also looked into the problem of obtaining the names of locations within the radius of a point for the purpose of obtaining relevant population information necessary for estimating market size. Two processes are important in helping to accomplish this task: geocoding and reverse geocoding. Geocoding simply means being able to take an address or name of a location and return a set of coordinates fo that location. Reverse geocoding is its opposite: it takes a set of coordinates as input an returns an address in response.\nOne package that I found to be useful while working on this problem was revgeo. Revgeo is an R package for reverse geocoding that returns the city, state, country and zip code of a set of coordinates provided by the user.\n\nrevgeo_address &lt;- function(longitude, latitude) {\n  \n  API_KEY &lt;- Sys.getenv(\"GOOGLE_API_KEY\")\n  \n  address &lt;- revgeo::revgeo(longitude = longitude, latitude = latitude, \n                            provider = 'google', output = \"frame\",\n                            API = API_KEY)\n  \n  address\n}\n\naddress &lt;- revgeo_address(-92,41)\n\n\naddress\n\nNULL\n\n\nThis package proved to be useful for making a basic function that gets the list of nearby counties within a radius of a set of coordinates. However, the biggest flaw is that the “county” column isn’t accurate; it lists the name of the country instead. For our purposes, this was a significant roadblock, as having the county of a point makes retrieving data from TidyCensus a lot easier.\nOne issue we’ve confronted with collecting data from geocoding is how to deal with coordinates near a state boundary. Because of the way that the TidyCensus library collects geography information, getting data for areas outside of state lines is not a straightforward process. Therefore, we have been brainstorming ways to get this interstate data without placing too high of a demand on memory and performance."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#google-maps-api",
    "href": "posts/Week_6/Week-6.html#google-maps-api",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Google Maps API",
    "text": "Google Maps API\nWe used the Google Maps API to find the nearest grocery stores. An API is a tool developers provide for other developers that allows them to incorporate their work into other works. The API gives us the latitude and longitude of the other stores, which we can use to find the distance between the stores. We can also use these coordinates to generate polygons."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#quarter-circle",
    "href": "posts/Week_6/Week-6.html#quarter-circle",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Quarter Circle",
    "text": "Quarter Circle\nThe quarter circle was the original design in the excel spreadsheet. Quarter circles are very simple to implement, and work by finding the radius to the nearest store and turning it into a quarter circle that we are using as the region of our market share. The disadvantage of doing it this way is that it is a massive over estimate because it doesn’t factor the other store’s “sphere of influence” into account. These are essentially a rudimentary version of a Voronoi polygon."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#automating-population-data",
    "href": "posts/Week_6/Week-6.html#automating-population-data",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Automating Population Data",
    "text": "Automating Population Data\nWe are able to use census information to get data at various different resolutions, such as state, county, tract, and place. Using this we can generate maps and map regions to do different types of analysis. One example is knowing how many people live in a region between two grocery stores.\n\nCounty: We have census data for the populations of counties, which we can retrieve using TidyCensus. TidyCensus is a package for R which allows developers to easily get Census data.\nTown: Using the script Jay made, we can find the populations of all the towns in a given geometry. Jay’s script uses a package called Tigris, which allows us to see demographic data within a geographic boundary. Jay’s script has functions that allow you to give a latitude, longitude, and state, and it will return the city name. You can also give it a county and have it return all the towns in that county.\nRural: We are in the process of automating the rural population. We can calculate this by adding up the populations of all the towns in the county and subtracting that from the total number of people in the county."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#voronoi",
    "href": "posts/Week_6/Week-6.html#voronoi",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Voronoi",
    "text": "Voronoi\nVoronoi polygons are a way of breaking up a plane into sections defined by which node of a graph they are closest to. This shows a model where people will go to the closest grocery store purely based on their location. This doesn’t take into account anything about the store, and only cares about the location. The pro’s of this are that in rural settings, people are most likely going to go to the nearest store. The cons of this are that it ignores factors such as offerings, pricing, and the fact that people tend to go towards more densely populated areas to shop.\n\nVoronoi Polygon demonstration from Wikipedia\n\nVoronoi diagram of grocery stores in Iowa"
  },
  {
    "objectID": "posts/Week_6/Week-6.html#reillys-law",
    "href": "posts/Week_6/Week-6.html#reillys-law",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Reilly’s Law",
    "text": "Reilly’s Law\nReilly’s law of retail gravitation is the idea that people are drawn to shop in areas with denser populations. The formula for Reilly’s law is a ratio of distance between cities and the difference in the populations between the two. This is a good model because it takes into account the shopping habits of consumers. The downsides of this model are that it has a lot of limitations, such as the cities having to have relatively similar populations, and it gives an overestimate because it assumes everyone shops locally."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#voronoi-vs-reillys-law",
    "href": "posts/Week_6/Week-6.html#voronoi-vs-reillys-law",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Voronoi vs Reilly’s Law",
    "text": "Voronoi vs Reilly’s Law\nVoronoi and Reilly’s law can tell us similar, but different things. Voronoi shows the geographic regions where our proposed store would be the closest. Reilly’s law would show the regions around our store that people would be willing to travel in order to shop at the store."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#difficulty-in-the-process",
    "href": "posts/Week_6/Week-6.html#difficulty-in-the-process",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Difficulty in the process",
    "text": "Difficulty in the process\nFiguring out the State index and percentage price increase values took a lot of my time after I created the functions. I shortlisted some of the resources that I found can be useful, like the CPI(Consumer Price Index), PPI(Producer Price Index), RPP(Rural Price Parities), and Cost of Living index. What made it more confusing for me was that some of these had values specifically for groceries other than the overall value. After discussing with Duane(our client) in our last meeting, I chose to use the Consumer Price index, which is the measure of the average change over time in the prices paid by urban consumers for a market basket of consumer goods and services, as the estimated price increase, and use Rural Price Parities, which measure the differences in price levels across states and metropolitan areas for a given year and are expressed as a percentage of the overall national price level, as the State Index."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#update-and-maintenance",
    "href": "posts/Week_6/Week-6.html#update-and-maintenance",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Update and Maintenance",
    "text": "Update and Maintenance\nI created the following table to Track the source and links about the variables used and how frequently it is required to be updated.\n\n\n\n\n\n\n\n\n\n\nVariable name\nFrequency\nSource\nLink\nNotes\n\n\nTotal US Grocery Sales\nOptional\nIBIS\n\nDefault base year taken as 2022\n\n\nTotal US population\nOptional\nUS Census Bureau\n\nDefault base year taken as 2022\n\n\nEstimated cumulative price increase(CPI)\nyearly update/ Half yearly update\nUS Bureau of labor statistics\nhttps://data.bls.gov/timeseries/CUUR0000SA0\nCPI in current year - CPI in base year\nFor now defaulting as 7 for 2023\n\n\nState Index\nyearly update\nBEA\nhttps://tinyurl.com/ycwpjzwz\nDepends on state\n\n\n\nThe consumer Price index is updated every month, but it is fine to take yearly once or twice because it does not change each month very much. And for the Rural Price parities, the latest data that we have is 2021 because it is economic census data and is not available for the recent 1 year."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#sales-genie-data-set-vs-google-api-data-set",
    "href": "posts/Week_6/Week-6.html#sales-genie-data-set-vs-google-api-data-set",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "Sales Genie Data Set Vs Google API data set",
    "text": "Sales Genie Data Set Vs Google API data set\nI was working with the Sales Genie Data sets about the grocery stores and dollar stores in Iowa. I also looked at the Google API data set for the dollar store data in Iowa and Illinos. Most of the visualizations that I created were using the sales Genie Data Sets. Although one problem that I noticed was that it could possibly be missing some stores. I noticed this when I was trying to plot the number of stores in each Dollar Store chain using both the Google API data set and the Sales Genie Data set.\n\nThe above plot shows the number of cities in Iowa with more than 1 dollar store plotted using the Sales Genie Data Set. We notice that Ames is not there, although there are 2 dollar stores in Ames.\nWhen I saw the Google API data set to check this, it was also incorrect, saying there are 4 dollar stores in Ames. When I looked at the data table, I found that it had a store which was near Ames, which had the mailing city as Ames. I have more difficulty with the Google data set is that it is not cleaned it has some dollar generals as DG market and differentiated based on name than on chain like shown in the below plot:\n\nThe following plots show the distribution of stores in cities of Iowa with more than one chain grocery store and non-chain grocery store, respectively. I am currently working on improving this plot by trying to make a single plot that contains the distribution of the stores color-coded by the type so that we can see if there are any trends that we can observe about the presence of one type of store affecting the other(like if dollar stores affect the non-chain grocery stores).\n\n\nFrom all these plots, one common thing that we can observe is that irrespective of the type, there are usually more stores in big cities than in small cities, as we would expect.\n\nSales Volume Distribution\n\nlibrary(ggplot2)\nsales_chain_grocery &lt;- ggplot(data = chain_grocery_store_data,\n                              aes(x = Location_Sales_Volume_Range,\n                                  fill = Company_Name)) +\n            geom_bar(position = \"stack\") +\n  ggtitle(\"Location Sales Volume distribution for Chain Grocery Stores\") +\n  xlab(\"Sales Volume Range\") + \n  ylab(\"Number of Stores\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\nggplotly(sales_chain_grocery)\n\n\nThe above plot shows the distribution of the number of stores by Sales volumes for the major chain grocery stores in Iowa. I must do data cleaning and arrange the x-axis titles in ascending order to make them more interpretable.\n\nThe plot shows the location sales volume for the non-chain grocery stores. We can see that many of the non-chain grocery stores have a 1-2.5 Million sales volume."
  },
  {
    "objectID": "posts/Week_6/Week-6.html#rucc",
    "href": "posts/Week_6/Week-6.html#rucc",
    "title": "Grocery Team Weekly Wrap-Up: Week 6",
    "section": "RUCC",
    "text": "RUCC\nThe 2013 Rural-Urban Continuum Codes form a classification scheme that distinguishes metropolitan counties by the population size of their metro area and non metropolitan counties by the degree of urbanization and adjacency to a metro area.\nEach county in the U.S. is assigned one of the nine codes listed below. Codes 4-9 are typically considered to be rural.\n\nRUCC 1: Counties in metro areas of 1 million population or more\nRUCC 2: Counties in metro areas of 250,000 to 1 million population\nRUCC 3: Counties in metro areas of fewer than 250,000 population\nRUCC 4: Population of 20,000 or more, adjacent to a metro area\nRUCC 5: Population of 20,000 or more, not adjacent to a metro area\nRUCC 6: Population of 2,500 to 19,999, adjacent to a metro area\nRUCC 7: Population of 2,500 to 19,999, not adjacent to a metro area\nRUCC 8: Less than 2,500 population, adjacent to a metro area\nRUCC 9: Less than 2,500 urban population, not adjacent to a metro area\n\nMatching the city names in the Sales Genie Data Set to their counties will help us make a plot classifying the stores based on their location according to the RUCC (Rural-Urban Continuum Codes). This could help us visualize if there is any correlation between the number of a particular type of store and urban or rural counties"
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#list-of-data-sources",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#list-of-data-sources",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "List of Data Sources",
    "text": "List of Data Sources\n\nAmerican Community Survey\nU.S Decennial Census\nSalesgenie\nGoogle Places"
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#average-grocery-spend",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#average-grocery-spend",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Average Grocery Spend",
    "text": "Average Grocery Spend\nThe average per capita grocery spend is the average money each person in the US spends on groceries per year. This value acts as a baseline for how much we can expect each shopper to spend in a store. We created three functions that calculate this value and adjust for inflation.\nFirst, we calculate the average per capita grocery spending for 2022 by dividing the total grocery sales from IBIS by the total US population as of 1/1/2023. This is done by the function Avg_Capita_Grocery_Spend() . Since it is calculated for 2022, it should be adjusted for inflation from the base year(2022). CPI and RPP does this adjustment to the value in the functions Adj_Capita_Grocery_Spend() and State_Adj_Capita_Grocery_Spend(), respectively.\n\nConsumer Price Index(CPI): Measure of the average change over time in the prices paid by urban consumers for a market basket of consumer goods and services. So, adjusting according to the CPI accounts for the overall inflation in the US.\nRegional Price Parities(RPP): Measure the differences in price levels across states and metropolitan areas for a given year and are expressed as a percentage of the overall national price level. This value is 100 for the overall US, and it is lesser or higher than 100 depending on if the state’s price levels are lower or higher than the national average. Adjusting according to this measure gives that state’s average per capita spending on groceries for that year.\nUpdate and Maintenance\n\n\nMoney spent by different categories of shoppers:\n\n\nVariable name\nFrequency\nSource\nLink\nNotes\n\n\nTotal US Grocery Sales\nOptional\nIBIS\n\nDefault base year is taken as 2022\n\n\nTotal US population\nOptional\nUS Census Bureau\n\nDefault base year is taken as 2022\n\n\nEstimated cumulative price increase(CPI)\nYearly update/ Half yearly update\nUS Bureau of Labor Statistics\nhttps://data.bls.gov/timeseries/CUUR0000SA0\nCPI in the current year - CPI in the base year\nFor now, defaulting as 7 for 2023\n\n\nState Index\nYearly update\nBEA\nhttps://tinyurl.com/2wvca7vy\nShould be inputted by the user according to the store location.\n\n\n\nSince not all shoppers will have the same level of preference for all stores, it is important to consider and add weight to the proportion of money spent. To do this, the shoppers are classified into:\n\nPrimary Shoppers: The people doing most of the household grocery shopping in that store.\nSecondary Shoppers: Regular visitors for smaller purchases but will often do their weekly shopping elsewhere.\nRare Shoppers: People who are very occasional shoppers(only when necessary).\nShopper choosing a store as a primary store or not depends on many factors, one of the main ones being distance and accessibility. So we classify the markets based on distance or location into:\nMetro: The population of the city in which the store is planned to open\nTown: Population of all the cities in the market\nRural: Total rural population of the market\n\nFrom the above classifications, it would be reasonable to expect more primary customers from metro markets than from elsewhere because of the proximity. So the weight is added using the default percentages of primary, secondary, and rare shoppers from the different markets.\n\n\n\n\nPrimary\nSecondary\nRare\n\n\n\n\nMetro\n50%\n40%\n10%\n\n\nTown\n30%\n50%\n20%\n\n\nRural\n30%\n50%\n20%\n\n\n\nThe total number of primary, secondary, and rare shoppers is calculated by multiplying the corresponding percentage with our market’s metro, town, and rural population calculated in the previous section. This calculation is done in the functions Primary_Shoppers_Count() , Secondary_Shoppers_Count() , and Rare_Shoppers_Count() .\nAs Primary shoppers do most of their shopping in that grocery store, the percentage of the average they spend(60%) is much more than that spent by the secondary shoppers(25%) or rare shoppers(5%). The total spend for each category of shoppers is calculated individually by the functions Total_Spend_Primary_Shoppers() , Total_Spend_Secondary_Shoppers() , and Total_Spend_Rare_Shoppers() by multiplying the percentage, number of shoppers, and average grocery spend."
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#total-revenue",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#total-revenue",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Total Revenue",
    "text": "Total Revenue\nThe function, Total_Estimate_Revenue() , calculates the final total estimated revenue by summing the outputs from the Total_Spend_Primary_Shoppers() , Total_Spend_Secondary_Shoppers() , and Total_Spend_Rare_Shoppers() ."
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#grocery-sales-visualizations",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#grocery-sales-visualizations",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Grocery Sales Visualizations:",
    "text": "Grocery Sales Visualizations:\nAs discussed before, a shopper choosing a store as a primary store or not depends on a lot of other factors like the competition from the big chain grocery stores, other non-chain grocery stores, and dollar stores, which provide them a cheaper alternative. So to understand how the store location and presence of competitors affect a store, we analyzed the Sales Genie’s data on dollar stores and grocery stores in Iowa. We classified the cities in our data set according to the city group classification based on population and adjacency to a big city. This classification classifies the cities into these eight groups:\n\nGroup 1: 10,000 or greater Core county of a metropolitan statistical area (MSA) \nGroup 2: 10,000 or greater Non-core MSA county or non-metropolitan county \nGroup 3: 2,500 to 9,999 Non-metropolitan county \nGroup 4: 2,500 to 9,999 Metropolitan county \nGroup 5N: 500 to 2,499 Non-metropolitan county, not adjacent to an MSA\nGroup 5A: 500 to 2,499 Non-metropolitan county, adjacent to an MSA \nGroup 6: 500 to 2,499 Metropolitan county \nGroup 7: 250 to 499. Any county\nRest of State: 249 or fewer Any county \n\nOf these, we specifically focused on the cities with a population of less than 2500 and created a plot that shows the number of the different types of stores in each city group classification.\n\nWe see that there are very less chain grocery stores in call these cities with a population of less than 2500. As we go more and more rural, the only competition in the city is from the other non-chain grocery stores in that area. The distribution of the sales volume for various non-chain grocery stores is also shown in the dashboard. These visualizations will help small rural grocery business owners to design their stores depending on the preexisting competition."
  },
  {
    "objectID": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#updates-and-maintenance",
    "href": "posts/Final_Presentation_Grocery/Final_Presentation_Grocery.html#updates-and-maintenance",
    "title": "Using Data to Inform Decision Making for Rural Grocery Stores",
    "section": "Updates and Maintenance",
    "text": "Updates and Maintenance\nSince the Bizminer/Vertical IQ percentages used in the calculations for the functions in this section are based on three-year averages, they will need to be updated annually."
  }
]