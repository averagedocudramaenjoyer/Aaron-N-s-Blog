---
title: "Grocery Team Weekly Wrap-Up: Week 6"
author: "Aaron Null, Alex Cory, Srika Raja and Harun Celik"
date: "2023-06-22"
categories: "Week Six"
---

![Lunchtime at Grundy Center](/posts/Week_6/images/lunchtime.jpg){width="545"}

# Overview

The primary focus for this week was understanding how all of the work that we did for our particular sections of the project will fit together into a seamless whole. In order to do that, we have been learning more about each other's work done toward the beginning of the program and thinking about ways to connect them in the context of the final application.

We also spent the first part of the week collecting data for the WINVEST project. On Monday we visited Grundy Center and New Hampton, while on Tuesday we visited Independence, all of which were northeast of Ames. We spent time walking through residential areas taking pictures of houses and filling out forms to be compiled into a dataset that will be analyzed later for the purpose of determing which of the areas will qualify for a grant from the state. We saw many interesting sites and learned a lot of doing field work for data science.

![A house that would be marked as "good".](/posts/Week_6/images/housegood.png){width="505"}

![A house that would be marked as being in "poor" condition.](/posts/Week_6/images/housepoor.png){width="508"}

# Current Project Objectives

One important feature of the project is how we measure market size. This involves getting an estimate of the population around a user-defined point in the application. We have been working on the shape and orientation of this area, as well as how to load only the data that we need in response to user-input. Our estimated customer base in our project is influenced by multiple factors, including the presence of dollar stores, other grocery stores, whether the population is urban or rural and other things. This data ultimately factors into our calculation for estimated revenue of the proposed store, which factors into the calculation of profitability. As we get closer and closer to our deadline, we are thinking more and more about our final tool and what will and will not make it into the final draft. We are hopeful that we will be able to balance user-friendliness with depth to make a tool that's easy to use and valuable for our users.

## Aaron

### The Percentage Question

One important concern for the expenses component of the project is the question of modularity, or the freedom given to the user to affect the calculations performed in the server. The functions used in this project to calculate expenses for potential grocery stores do so by providing different percentages of revenue spent on expenses based off of how high the total estimated revenue input is at the beginning. The original excel sheet establishes 5 distinct ranges that each have different percentages of the revenue across the different line items of the sheet.

![Percentages for categories of expenses/income across revenue ranges/store sizes (Bizminer)](/posts/Week_6/images/percentages_blog-01.png)

These ranges can be thought of as stand-ins for the different sizes of grocery stores. When a store takes in a certain amount of revenue, the function assigns a budget percentage corresponding to the assumed size of the store.

As it stands now, many of the functions make use of nested "ifelse" statements in order to find the budget percentage of an expense or a source of income and multiply it by the total estimated revenue. For example:

```{r, eval=FALSE}

Employee_Wages <- function(Total_Estimated_Revenue) {

  ifelse(Total_Estimated_Revenue < 500000, stop("error: no data for this revenue range"),
         percentage <- ifelse(Total_Estimated_Revenue < 999999.99, .0789,
                              ifelse(Total_Estimated_Revenue < 2499999.99, .0934,
                                     ifelse(Total_Estimated_Revenue < 4999999.99, .0751,
                                            ifelse(Total_Estimated_Revenue < 24999999.99, .0975, .1083)))))

  Total_Estimated_Revenue * percentage
}
```

The function gives the user an error message if they enter a revenue amount under \$500,000 dollars due to there being no data at that range.

### The Linearity Problem

Many of the functions derived from the excel sheet "Estimating Expenses" have been written to take the total estimated revenue as an input and return the dollar amount of a certain cost in accordance to the average percentage of revenue spent on that cost for stores of a similar size. For example, a store that takes in \$1,000,000 and a store that takes in \$2,000,000 share common percentages for costs such as employee wages and officer compensation. This broad assignment of one percentage based on a large revenue window also applies to the function for calculating gross margin, or the amount of funds left over after accounting for the cost of goods for the store.

However, since most of these functions receive the same input of total estimated revenue and all of the percentages for the expenses are bundled together for the same window of revenue, this creates a problem. Suppose we want to compare the profitability of a store that takes in \$1,000,000 of revenue vs. \$2,000,000. According to the way our calculations work now, both of these stores will have the same percentage of their revenue deducted as expenses. So if 95% of our revenue goes towards covering expenses for every value that falls within the window of \$1,000,000 to \$2,500,000, there is a perfect linear relationship between the revenue and the profit of the store; the \$1,000,000 store collects \$50,000 in profit while the \$2,000,000 store collects \$100,000. If we were to create a numeric regular sequence of revenue inputs from \$1,000,000 to \$2,500,000 and plot it against one of our measures of profit (before depreciation etc.), we would get:

![Revenue vs. Expense](/posts/Week_6/images/revenue_and_expense-02.png){width="597"}

This is a problem because the ultimate goal of our tool is to help potential grocery store owners decide which area could be more profitable for the store that they want to open than the next area. In other words, the model doesn't afford any insight into the relative advantage of setting up a store in a given a area versus another, because any increased revenue from a chosen area is immediately offset by a corresponding increase in expenses that affects every store within a revenue range uniformly.

### A Potential Solution

One way to ameliorate this problem is to simply give more control to the user of the application in choosing their own expense percentages. That way, the user can tinker with the tool in order to determine (in a broad way) how much resources they must have and how they must allocate those resources in order to be profitable in a certain location. We propose that this can be done through R Shiny. R Shiny allows for numeric inputs into functions by means of a user-controlled slider. Through this method, there is no need to discard the default percentages that we were previously working with; rather, instead of being hard-coded into the function, they can be placed as labels along the slider or set as default values that the user can adjust manually if they so choose.

![Slider input example (Shiny)](/posts/Week_6/images/slider_input-01.png)

This way, the user can know the industry averages for different variables while also comparing their own plans with those averages.

### The Rounding Problem

Another issue I faced was one involving the rounding of percentages of my functions. I had rounded the original percentages up or down depending on whether they were considered a cost or an expense in the balance sheet, and this affected my output significantly when I went to compare my R output to the example provided by Duane (our client) in his excel sheet. Thankfully, when I reverted the percentages to their original state (4 decimal places), my outputs then matched the one's displayed in the excel sheet. The big takeaway from this is that most of the time, the final output is what should be rounded in a calculation and not the intermediaries. I mistakenly thought that rounding the percentages used in the calculation in certain directions could provide a more "conservative" or "risky" weight to the final prediction. However, since we were dealing with small percentages of very large numbers, the degree of rounding that was there greatly impacted overall accuracy and rendered the predictions illogical. Thankfully, the functions work as they are supposed to now (even thought they may undergo a significant rewrite later).

### Geocoding/Reverse Geocoding

Alongside working on the expenses functions, I also looked into the problem of obtaining the names of locations within the radius of a point for the purpose of obtaining relevant population information necessary for estimating market size. Two processes are important in helping to accomplish this task: *geocoding* and *reverse geocoding.* Geocoding simply means being able to take an address or name of a location and return a set of coordinates fo that location. Reverse geocoding is its opposite: it takes a set of coordinates as input an returns an address in response.

One package that I found to be useful while working on this problem was *revgeo*. Revgeo is an R package for reverse geocoding that returns the city, state, country and zip code of a set of coordinates provided by the user.

```{r, results='hide', message=FALSE, warning=FALSE}
revgeo_address <- function(longitude, latitude) {
  
  API_KEY <- Sys.getenv("GOOGLE_API_KEY")
  
  address <- revgeo::revgeo(longitude = longitude, latitude = latitude, 
                            provider = 'google', output = "frame",
                            API = API_KEY)
  
  address
}

address <- revgeo_address(-92,41)
```

```{r}
address
```

This package proved to be useful for making a basic function that gets the list of nearby counties within a radius of a set of coordinates. However, the biggest flaw is that the "county" column isn't accurate; it lists the name of the country instead. For our purposes, this was a significant roadblock, as having the county of a point makes retrieving data from TidyCensus a lot easier.

More on the process of geocoding will be found below.

## Grocery Survey: First Impressions

We have also been looking over the results of a survey conducted by Iowa State Extension and Outreach (sponsored by AgMRC) that asked owners and senior managers of independently-owned rural grocery stores throughout Iowa a series of questions about their store's performance, selection and design, as well as the top challenges they face in keeping their store profitable. The survey divided Iowa into 3 regions and examined the survey results accordingly:

![Three regions analyzed by the survey.](/posts/Week_6/images/iowa_regions.png){width="626"}

One issue with the survey was the low response rate: only 95 out of 671 independent grocers responded (14.2%). However, the findings might still be useful to look at for our purposes.

Here are some charts I found interesting:

![Top challenges faced by small grocers in Iowa](/posts/Week_6/images/challenges.png){width="628"}

![Difficulties in purchasing and selling local products](/posts/Week_6/images/selling_local.png){width="641"}

There is much more potentially useful information, such as information about the most frequently found unique assets of stores and the specific types of local products most frequently bought according to region, shown here:

![Type of local foods purchased by region](/posts/Week_6/images/local_foods_bought.png){width="587"}

Overall, this survey may help us provide useful information to the users of our tool and can help grant us perspective on how well the functionality of our tools maps onto the ground truth of what is really going on with rural grocery stores in Iowa. I found it to be a very interesting and useful resource.
