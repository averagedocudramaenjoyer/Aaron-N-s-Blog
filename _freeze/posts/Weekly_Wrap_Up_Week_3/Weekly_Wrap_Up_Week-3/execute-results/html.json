{
  "hash": "d6ce69fb8c71138ac15b1c0487f9548c",
  "result": {
    "markdown": "---\ntitle: \"Grocery Weekly Wrap Up\"\nauthor: \"Aaron Null, Alex Cory, Srika Raja and Harun Celik\"\ndate: \"2023-06-02\"\ncategories: \"Week Three\"\n---\n\n\n# Current Project Objectives\n\nThis week was primarily centered on clarifying the focus of our project, as well as on the retrieval of relevant sources of data. During the first part of the week, our group searched TidyCensus, the USDA and various other internet sources for data that could potentially be useful for whichever type of tool that we and our client decide on.\n\nIt could be said that we have started moving into the second phase of our project this week: data collection.\n\n![](images/MicrosoftTeams-image-01.png)\n\nOn Thursday, we met with our client and some of his associates at the the research park to discuss the direction of the project. We had them answer various questions on Mentimeter and showed them the list of datasets under consideration. They also gave us numerous suggestions about sources they were familiar with and told us that they would help grant us access to those of which were not immediately available to us.\n\n![](images/IMG_1911.jpg)\n\n![](images/IMG_1912.jpg)\n\nHere are some of the Mentimeter results from our meeting:\n\n![](images/menti%201.jpg)\n\n![](images/menti%20a.jpg)\n\n![](images/menti%20%25.jpg)\n\nAs we are wrapping up this week, we have now shifted our focus to compiling our selected data to our repository and filtering through sources that may not be as useful as others. We are continuing to think of ways to optimize this process, as well as remaining open to other sources of data or information that we have yet to discover. Here are some of the resources under consideration:\n\n![](images/Acs_slide.jpg)\n\n![](images/USDA_slide.jpg)\n\n![](images/Econ_census_slide.jpg)\n\n# Works in Progress\n\n## Aaron\n\nThis week has been less coding-centric than the previous week, as many of our efforts have been centered on uncovering useful sources of data. However, I have tried to work through ways to accelerate the time-consuming process of labeling and pushing CSVs of tables of ACS data.\n\nHere is an example of such a table: \"Mode of Transport to Work by County 2021\"\n\n![](images/dataaaa.png)\n\nI have also written a function that I've been using to speed the process up. It's a data labeler function that takes \"table\", \"year\" and \"geography as arguments and outputs a labelled ACS5 data table by joining labels from the\"load_variables()\" function in TidyCensus by the common variable code on both dataframes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidycensus)\nlibrary(stringr)\nlibrary(readr)\n\n\n\ndata_labeler_Iowa <- function(table, year, geography){\n  df <- tidycensus::get_acs(geography = geography,\n                table = table,\n                state = \"IA\",\n                year = year,\n                survey = \"acs5\")\n  \n  \n  vars <- tidycensus::load_variables(year, \"acs5\")\n  \n  vars <- vars %>%\n    dplyr::filter(stringr::str_detect(name, table))\n  \n  vars <- vars %>% \n    dplyr::rename(variable = name)\n  \n  vars <- vars %>%\n    dplyr::select(variable, label)\n  \n  df2 <- merge(df,vars)\n  \n  df2 %>%\n    dplyr::arrange(GEOID)\n}\n\nMedian_Household_Income_County_2021 <- data_labeler_Iowa(\"B19013\", 2021, \"county\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nGetting data from the 2017-2021 5-year ACS\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Median_Household_Income_County_2021)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    variable GEOID                   NAME estimate  moe\n1 B19013_001 19001     Adair County, Iowa    57944 4047\n2 B19013_001 19003     Adams County, Iowa    57981 7853\n3 B19013_001 19005 Allamakee County, Iowa    59461 2644\n4 B19013_001 19007 Appanoose County, Iowa    46900 5645\n5 B19013_001 19009   Audubon County, Iowa    54643 5861\n6 B19013_001 19011    Benton County, Iowa    72334 3626\n                                                                                         label\n1 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n2 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n3 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n4 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n5 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n6 Estimate!!Median household income in the past 12 months (in 2021 inflation-adjusted dollars)\n```\n:::\n:::\n\n\nBuilding on this, I tried to iterate through all of these arguments by using nested for loops and writing csvs automatically. However, for a reason I don't entirely understand, I haven't gotten it to work the way I should. It's possible that this has to do with the limitations of the API call and its speed within the loop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyearlist <- c(2009, 2012, 2016, 2021)\ngeo_list <- c(\"county\", \"tract\")\nIncome_table_list <- c(\"B19013\", \"B19113\", \"B19202\", \"B19051\", \"B19055\")\nacs_table <- NULL\n\n\nfor(table in Income_table_list)\n\n  for(year in yearlist)\n  \n    for(geography in geo_list)\n    \n      acs_table <- tidycensus::get_acs(geography = geography,\n                                       table = table,\n                                       state = \"IA\",\n                                       year = year,\n                                       survey = \"acs5\")\n      \n      write_csv(acs_table, file = sprintf(\"%s_%s_%s\", table, year, geography))\n```\n:::\n\n\nHowever, even if this didn't completely work, I learned a lot of useful information, such as the \"sprintf\" function that provides placeholders for strings that are named from iterating variables in loop, as well as how to call only necessary elements of a library into a function to prevent using too much memory/slowing run time.\n\n## Alex\n\n### Data Exploration\n\nI spent a lot of time this week doing data exploration. I found the USDA food atlas data set, as well as the Economic Census data. I also explored the TinyUSDA package.\n\n### Client Meeting\n\nThis week I met with our clients, where we clarified the scope of the project, discussed the goals moving forwards, and shared our progress. I shared about the USDA food atlas and Economic Census data sets\n\n### SQL learning\n\nThis week I spent time learning PostgreSQL on DataCamp. I also did several other courses related to data visualizations and fundamental statistical skills.\n\n## Srika\n\n### Training:\n\n#### Data Camp courses completed:\n\nIntroduction to statistics in R\n\nData visualization with R (in progress)\n\n### Created summaries of the client reports:\n\nUnderstanding the market trends and margins for different departments in a rural grocery store.\n\n### Collected some ACS Data\n\nBrowsed for other possible useful sources to consider\n\n![](images/srika_blog_image.png)\n\n# DSPG Questions\n\n-   What is the best way to scrape text off of a webpage with R?\n\n-   What are some general rules/best practices for writing efficient and reliable functions?\n\n-   What is differential privacy?\n\n## \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}